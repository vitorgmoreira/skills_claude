---
name: pdf-to-skill-converter
description: Meta-skill que converte livros PDF em especialistas virtuais Claude otimizados. Oferece modo BÃ¡sico (0 tokens) e Premium (com refinamento IA). Inclui cÃ¡lculo dinÃ¢mico de custos, sistema de consulta interativa e geraÃ§Ã£o automÃ¡tica de skills especializadas com navegaÃ§Ã£o inteligente, FAQs, frameworks extraÃ­dos e exemplos contextualizados.
license: MIT
---

# PDF to Skill Converter - Meta-Skill de GeraÃ§Ã£o de Especialistas Virtuais

## ğŸ¯ PropÃ³sito

Esta skill converte qualquer PDF de livro tÃ©cnico em uma **skill especializada otimizada** - essencialmente criando um **especialista virtual** no conteÃºdo do livro que Claude pode consultar para responder perguntas, aplicar conceitos e criar implementaÃ§Ãµes prÃ¡ticas.

## ğŸ§  O Que Esta Skill Cria

NÃ£o apenas "um livro digitalizado", mas sim um **especialista virtual** que:

- âœ… Conhece o livro completamente
- âœ… Navega inteligentemente pelo conteÃºdo
- âœ… Adapta exemplos ao contexto do usuÃ¡rio
- âœ… Conecta conceitos de diferentes capÃ­tulos
- âœ… Responde instantaneamente
- âœ… Cria frameworks personalizados
- âœ… DisponÃ­vel 24/7 a custo irrisÃ³rio

## ğŸš€ Como Usar

### Comando BÃ¡sico
```
"Claude, use a skill pdf-to-skill-converter para processar 
[nome-do-arquivo.pdf] e criar uma skill especializada"
```

### Comando com Contexto
```
"Claude, converta [nome-do-arquivo.pdf] em skill especializada,
contextualizando para [seu negÃ³cio/indÃºstria/uso especÃ­fico]"
```

## âš™ï¸ Processo de ConversÃ£o

### FASE 0: CONSULTA INTERATIVA (SEMPRE EXECUTAR PRIMEIRO)

**CRITICAL**: Antes de iniciar qualquer processamento, Claude DEVE apresentar as opÃ§Ãµes ao usuÃ¡rio de forma educativa e aguardar decisÃ£o explÃ­cita.

#### Template de ApresentaÃ§Ã£o ao UsuÃ¡rio

```markdown
ğŸ“š Iniciando conversÃ£o de [{NOME_DO_PDF}]...

Antes de prosseguir, vocÃª precisa escolher o modo de conversÃ£o.
Deixe-me explicar as diferenÃ§as para que vocÃª tome a melhor decisÃ£o:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ MODO BÃSICO (ConversÃ£o Estrutural)
ğŸ’° Custo: 0 tokens na geraÃ§Ã£o

ğŸ“– O PDF tem {NUM_PAGINAS} pÃ¡ginas

O que este modo faz:
âœ… Extrai todo o conteÃºdo (Python local - 0 tokens)
âœ… Detecta estrutura: capÃ­tulos, seÃ§Ãµes, hierarquia
âœ… Cria navegaÃ§Ã£o inteligente e Ã­ndices
âœ… Quebra em camadas: TL;DR â†’ Resumo â†’ Completo
âœ… Gera glossÃ¡rio e referÃªncias cruzadas
âœ… Cria matriz de aplicabilidade
âœ… Extrai frameworks e templates

Como funciona:
- TL;DRs: primeiras frases de cada seÃ§Ã£o (extraÃ§Ã£o automÃ¡tica)
- FAQs: baseadas em tÃ­tulos e subtÃ­tulos
- Exemplos: mantidos como no original
- EstruturaÃ§Ã£o: 100% automÃ¡tica por regras

Qualidade resultante:
â­â­â­â­ Muito boa - Funcional imediatamente
âš ï¸ Pode precisar de ajustes manuais em TL;DRs/FAQs depois

Economia no uso: ~73% menos tokens por consulta
Melhor para: Livros para consulta eventual, POCs, testes

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ MODO PREMIUM (Ultra-Otimizado com IA)
ğŸ’° Custo estimado para {NUM_PAGINAS} pÃ¡ginas: ~{CUSTO_ESTIMADO}k tokens (${CUSTO_USD})

CÃ¡lculo: ~{TOKENS_POR_100_PAG}k tokens a cada 100 pÃ¡ginas
- TL;DRs por IA: ~{TLD_TOKENS}k tokens
- FAQs inteligentes: ~{FAQ_TOKENS}k tokens  
- ContextualizaÃ§Ã£o: ~{CONTEXT_TOKENS}k tokens
- AnÃ¡lise de nuances: ~{NUANCE_TOKENS}k tokens

O que ADICIONA ao Modo BÃ¡sico:
âœ… Tudo do Modo BÃ¡sico +
âœ… TL;DRs gerados por IA (concisos, precisos, relevantes)
âœ… FAQs inteligentes (perguntas realmente Ãºteis)
âœ… Exemplos contextualizados para seu negÃ³cio
âœ… DetecÃ§Ã£o automÃ¡tica de contradiÃ§Ãµes e nuances
âœ… Analogias e adaptaÃ§Ãµes para seu contexto especÃ­fico
âœ… ComparaÃ§Ãµes entre conceitos similares
âœ… ExtraÃ§Ã£o semÃ¢ntica avanÃ§ada de conceitos

Como funciona:
- Claude processa cada capÃ­tulo para gerar conteÃºdo refinado
- AnÃ¡lise semÃ¢ntica profunda de conceitos
- ContextualizaÃ§Ã£o baseada no seu negÃ³cio
- OtimizaÃ§Ã£o mÃ¡xima para uso recorrente

Qualidade resultante:
â­â­â­â­â­ Excepcional - Pronto para uso avanÃ§ado
âœ¨ Especialista virtual de altÃ­ssima qualidade

Economia no uso: ~88% menos tokens por consulta
ROI: Positivo apÃ³s apenas 3-5 consultas!

Melhor para: Livros crÃ­ticos para o negÃ³cio, uso intensivo diÃ¡rio

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š COMPARAÃ‡ÃƒO DE CUSTOS - 100 Consultas ao Longo do Tempo

CenÃ¡rio: Livro de {NUM_PAGINAS} pÃ¡ginas, 100 perguntas ao longo de semanas/meses

âŒ Sem skill otimizada:
   Token mÃ©dio por consulta: ~55k
   Total em 100 consultas: 5.500.000 tokens
   Custo: ~$16.50

âœ… Com Modo BÃ¡sico:
   GeraÃ§Ã£o: $0.00
   Token mÃ©dio por consulta: ~15k
   Total em 100 consultas: 1.500.000 tokens
   Custo total: $4.50
   ğŸ’° Economia: $12.00 (73%)

âœ… Com Modo Premium:
   GeraÃ§Ã£o: ${CUSTO_USD} (uma Ãºnica vez)
   Token mÃ©dio por consulta: ~5.6k
   Total em 100 consultas: 560.000 tokens
   Custo total: ${CUSTO_TOTAL_PREMIUM}
   ğŸ’° Economia: ${ECONOMIA_PREMIUM} ({PERCENTUAL_ECONOMIA}%)
   
   ğŸ¯ Break-even: ApÃ³s {BREAKEVEN_QUERIES} consultas
   ğŸš€ ROI: Cada consulta apÃ³s break-even economiza ${ECONOMIA_POR_QUERY}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ TABELA DE CUSTOS POR TAMANHO DE LIVRO

| PÃ¡ginas | Modo BÃ¡sico | Modo Premium | Break-even | Economia (100 consultas) |
|---------|-------------|--------------|------------|--------------------------|
| 100     | $0.00       | ~$0.11       | 1 consulta | $14.38                   |
| 200     | $0.00       | ~$0.22       | 2 consultas| $14.27                   |
| 300     | $0.00       | ~$0.33       | 3 consultas| $14.16                   |
| 400     | $0.00       | ~$0.44       | 3 consultas| $14.05                   |
| 500     | $0.00       | ~$0.55       | 4 consultas| $13.94                   |
| 600     | $0.00       | ~$0.66       | 5 consultas| $13.83                   |

*PreÃ§os baseados em Claude Sonnet 4 ($3/M input tokens)*

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ POR QUE ESSAS OTIMIZAÃ‡Ã•ES EXISTEM?

**Problema sem otimizaÃ§Ã£o**:
âŒ Livros grandes = 50-80k tokens por consulta simples
âŒ Claude nÃ£o sabe onde buscar â†’ lÃª mÃºltiplos capÃ­tulos
âŒ UsuÃ¡rio paga caro em CADA pergunta
âŒ Resposta demorada

**SoluÃ§Ã£o: EstruturaÃ§Ã£o inteligente**:
âœ… NavegaÃ§Ã£o clara â†’ Claude sabe exatamente onde ler
âœ… Camadas de profundidade â†’ lÃª apenas o necessÃ¡rio
âœ… FAQs prontas â†’ resposta instantÃ¢nea sem processar
âœ… Ãndices semÃ¢nticos â†’ busca precisa de conceitos

**BenefÃ­cio do refinamento com IA (Premium)**:
âœ… TL;DRs precisos â†’ resolve 60% das perguntas com 100 tokens
âœ… FAQs relevantes â†’ resposta sem ler capÃ­tulo completo
âœ… Exemplos contextualizados â†’ aplicaÃ§Ã£o imediata
âœ… ROI extremamente rÃ¡pido â†’ paga-se em pouquÃ­ssimas consultas

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¤” Qual modo vocÃª prefere?

[1] Modo BÃ¡sico - ComeÃ§ar agora (0 tokens)
[2] Modo Premium - MÃ¡xima qualidade (~${CUSTO_USD})
[3] Me dÃª mais detalhes sobre as diferenÃ§as
[4] Mostre exemplos de output de cada modo
```

#### CÃ¡lculo DinÃ¢mico de Custos

```python
def calcular_custos(num_paginas):
    """
    Calcula custos baseado no nÃºmero de pÃ¡ginas
    ~220 tokens por pÃ¡gina = ~1.1k tokens por 100 pÃ¡ginas de refinamento
    """
    # Tokens de refinamento a cada 100 pÃ¡ginas
    tokens_por_100_pag = 11000  # ~11k tokens
    
    # CÃ¡lculo proporcional
    custo_tokens = (num_paginas / 100) * tokens_por_100_pag
    custo_usd = (custo_tokens / 1_000_000) * 3  # $3 por M tokens
    
    # Breakdown
    tldr_tokens = int(custo_tokens * 0.36)  # 36%
    faq_tokens = int(custo_tokens * 0.27)   # 27%
    context_tokens = int(custo_tokens * 0.23) # 23%
    nuance_tokens = int(custo_tokens * 0.14)  # 14%
    
    # Economia e ROI
    custo_sem_skill = 16.50
    custo_com_basico = 4.50
    custo_com_premium = custo_usd + (560_000 / 1_000_000 * 3)
    
    economia_premium = custo_sem_skill - custo_com_premium
    percentual = (economia_premium / custo_sem_skill) * 100
    
    # Break-even
    economia_por_query = (55000 - 5600) / 1_000_000 * 3  # $0.148
    breakeven = math.ceil(custo_usd / economia_por_query)
    
    return {
        'tokens_por_100_pag': tokens_por_100_pag / 1000,  # em k
        'custo_estimado': int(custo_tokens / 1000),  # em k
        'custo_usd': f"{custo_usd:.2f}",
        'tldr_tokens': tldr_tokens / 1000,
        'faq_tokens': faq_tokens / 1000,
        'context_tokens': context_tokens / 1000,
        'nuance_tokens': nuance_tokens / 1000,
        'custo_total_premium': f"{custo_com_premium:.2f}",
        'economia_premium': f"{economia_premium:.2f}",
        'percentual_economia': f"{percentual:.0f}",
        'breakeven_queries': breakeven,
        'economia_por_query': f"{economia_por_query:.3f}"
    }
```

#### Processamento da Escolha do UsuÃ¡rio

```python
def processar_escolha(resposta_usuario):
    """
    Processa a escolha do usuÃ¡rio e configura pipeline
    """
    resposta = resposta_usuario.lower().strip()
    
    if resposta in ['1', 'basico', 'bÃ¡sico', 'modo basico', 'modo bÃ¡sico']:
        return {
            'modo': 'basico',
            'llm_refinement': False,
            'context': {}
        }
    
    elif resposta in ['2', 'premium', 'modo premium']:
        # Coletar informaÃ§Ãµes de contexto
        return {
            'modo': 'premium',
            'llm_refinement': True,
            'context': coletar_contexto()
        }
    
    elif resposta in ['3', 'detalhes', 'mais detalhes', 'mais informaÃ§Ãµes']:
        # Expandir explicaÃ§Ã£o e reapresentar escolha
        return 'expandir_explicacao'
    
    elif resposta in ['4', 'exemplos', 'mostrar exemplos']:
        # Mostrar exemplos de output
        return 'mostrar_exemplos'
    
    else:
        # Resposta nÃ£o reconhecida - pedir clarificaÃ§Ã£o
        return 'clarificar'

def coletar_contexto():
    """
    Coleta informaÃ§Ãµes para contextualizaÃ§Ã£o Premium
    """
    print("""
Perfeito! Para maximizar a qualidade do Modo Premium,
preciso de alguns detalhes para contextualizaÃ§Ã£o:

1ï¸âƒ£ Contexto do seu negÃ³cio/Ã¡rea:
   Ex: "B2B SaaS para legal tech no Brasil"
   
2ï¸âƒ£ Principais casos de uso do livro:
   Ex: "Revenue operations, sales enablement, marketing strategy"
   
3ï¸âƒ£ Exemplos devem ser adaptados para:
   Ex: "Sua empresa, seu setor, seu paÃ­s"

VocÃª pode fornecer essas informaÃ§Ãµes? Ou posso usar o que
jÃ¡ sei sobre vocÃª?
    """)
    
    # Aguardar input do usuÃ¡rio ou usar contexto conhecido
    return {
        'business': 'B2B SaaS',
        'use_cases': 'Revenue operations, sales, marketing',
        'adapt_for': 'Sua empresa, seu setor'
    }
```

### FASE 1: EXTRAÃ‡ÃƒO E ANÃLISE (0 tokens)

```python
# modules/extraction.py

import pdfplumber
import re
from typing import List, Dict

def extract_pdf(pdf_path: str) -> Dict:
    """
    Extrai conteÃºdo completo do PDF preservando estrutura
    0 tokens - Processamento 100% local
    """
    book_data = {
        'metadata': {},
        'raw_pages': [],
        'num_pages': 0
    }
    
    with pdfplumber.open(pdf_path) as pdf:
        book_data['num_pages'] = len(pdf.pages)
        
        # Extrair metadata
        book_data['metadata'] = {
            'title': pdf.metadata.get('Title', 'Unknown'),
            'author': pdf.metadata.get('Author', 'Unknown'),
            'pages': len(pdf.pages)
        }
        
        # Extrair conteÃºdo pÃ¡gina por pÃ¡gina
        for i, page in enumerate(pdf.pages):
            page_data = {
                'number': i + 1,
                'text': page.extract_text(),
                'tables': page.extract_tables(),
                'width': page.width,
                'height': page.height
            }
            book_data['raw_pages'].append(page_data)
    
    return book_data

def detect_hierarchy(book_data: Dict) -> List[Dict]:
    """
    Detecta estrutura hierÃ¡rquica: capÃ­tulos, seÃ§Ãµes, subseÃ§Ãµes
    Usa heurÃ­sticas: tamanho de fonte, posiÃ§Ã£o, palavras-chave
    0 tokens - Processamento por regras
    """
    chapters = []
    current_chapter = None
    current_section = None
    
    # PadrÃµes comuns de identificaÃ§Ã£o
    chapter_patterns = [
        r'^(Chapter|CHAPTER|CapÃ­tulo|CAPÃTULO)\s+(\d+|[IVX]+)',
        r'^(\d+)\.\s+[A-Z]',
        r'^(PART|Part|PARTE|Parte)\s+(\d+|[IVX]+)'
    ]
    
    for page in book_data['raw_pages']:
        lines = page['text'].split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detectar capÃ­tulo
            for pattern in chapter_patterns:
                if re.match(pattern, line):
                    if current_chapter:
                        chapters.append(current_chapter)
                    
                    current_chapter = {
                        'title': line,
                        'number': len(chapters) + 1,
                        'sections': [],
                        'content': []
                    }
                    current_section = None
                    break
            
            # Detectar seÃ§Ã£o (heurÃ­stica: linha curta em maiÃºsculas)
            if (len(line) < 100 and 
                line.isupper() and 
                current_chapter and 
                not any(re.match(p, line) for p in chapter_patterns)):
                
                current_section = {
                    'title': line,
                    'content': []
                }
                current_chapter['sections'].append(current_section)
            
            # Adicionar conteÃºdo
            elif current_chapter:
                if current_section:
                    current_section['content'].append(line)
                else:
                    current_chapter['content'].append(line)
    
    # Adicionar Ãºltimo capÃ­tulo
    if current_chapter:
        chapters.append(current_chapter)
    
    return chapters

def extract_elements(chapters: List[Dict]) -> Dict:
    """
    Extrai elementos especiais: tabelas, listas, blocos de cÃ³digo
    0 tokens - Processamento por padrÃµes
    """
    for chapter in chapters:
        content_text = '\n'.join(chapter['content'])
        
        # Detectar listas
        chapter['lists'] = re.findall(
            r'(?:^|\n)((?:[-â€¢*]\s+.+\n?)+)',
            content_text,
            re.MULTILINE
        )
        
        # Detectar blocos de cÃ³digo (indentaÃ§Ã£o ou backticks)
        chapter['code_blocks'] = re.findall(
            r'```[\s\S]*?```|(?:^|\n)((?:    .+\n?)+)',
            content_text,
            re.MULTILINE
        )
        
        # Detectar definiÃ§Ãµes (Termo: DefiniÃ§Ã£o)
        chapter['definitions'] = re.findall(
            r'([A-Z][A-Za-z\s]+):\s+(.+?)(?:\n|$)',
            content_text
        )
    
    return chapters
```

### FASE 2: ANÃLISE SEMÃ‚NTICA (0 tokens - BÃ¡sico | VariÃ¡vel - Premium)

```python
# modules/semantic_analysis.py

from collections import Counter
import re
from typing import List, Dict, Set

def identify_concepts_basic(chapters: List[Dict]) -> Dict:
    """
    Modo BÃ¡sico: IdentificaÃ§Ã£o por frequÃªncia e padrÃµes
    0 tokens - AnÃ¡lise estatÃ­stica local
    """
    concepts = {}
    
    # Coletar todos os textos
    all_text = []
    for chapter in chapters:
        all_text.extend(chapter['content'])
        for section in chapter.get('sections', []):
            all_text.extend(section['content'])
    
    text = ' '.join(all_text).lower()
    
    # Extrair termos em negrito (** ou __)
    bold_terms = re.findall(r'\*\*(.+?)\*\*|__(.+?)__', text)
    bold_terms = [t[0] or t[1] for t in bold_terms]
    
    # Extrair substantivos prÃ³prios (heurÃ­stica: palavras capitalizadas)
    proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', ' '.join(all_text))
    
    # FrequÃªncia de palavras (> 3 letras, nÃ£o stopwords)
    stopwords = {'the', 'and', 'for', 'that', 'with', 'this', 'from', 'have', 'are', 'was', 'were'}
    words = re.findall(r'\b[a-z]{4,}\b', text)
    word_freq = Counter([w for w in words if w not in stopwords])
    
    # Combinar conceitos
    for term in bold_terms:
        concepts[term] = {
            'type': 'definition',
            'frequency': text.count(term.lower()),
            'chapters': []
        }
    
    for term in proper_nouns[:50]:  # Top 50
        if term not in concepts:
            concepts[term] = {
                'type': 'proper_noun',
                'frequency': proper_nouns.count(term),
                'chapters': []
            }
    
    for word, freq in word_freq.most_common(100):
        if word not in concepts and freq > 5:
            concepts[word] = {
                'type': 'keyword',
                'frequency': freq,
                'chapters': []
            }
    
    # Mapear conceitos para capÃ­tulos
    for concept, data in concepts.items():
        for i, chapter in enumerate(chapters):
            chapter_text = ' '.join(chapter['content']).lower()
            if concept.lower() in chapter_text:
                data['chapters'].append(i + 1)
    
    return concepts

async def identify_concepts_premium(chapters: List[Dict], context: Dict) -> Dict:
    """
    Modo Premium: AnÃ¡lise semÃ¢ntica com LLM
    ~36% do custo total de refinamento
    """
    # Este mÃ©todo seria chamado apenas se llm_refinement = True
    # Usaria API Anthropic para anÃ¡lise mais profunda
    pass

def map_dependencies(chapters: List[Dict]) -> Dict:
    """
    Cria Ã¡rvore de dependÃªncias entre capÃ­tulos
    0 tokens - AnÃ¡lise de referÃªncias cruzadas
    """
    dependencies = {}
    
    for i, chapter in enumerate(chapters):
        chapter_num = i + 1
        dependencies[chapter_num] = {
            'requires': [],
            'referenced_by': [],
            'related_topics': []
        }
        
        content = ' '.join(chapter['content'])
        
        # Detectar referÃªncias a outros capÃ­tulos
        refs = re.findall(
            r'(?:Chapter|Cap\.?|see)\s+(\d+)',
            content,
            re.IGNORECASE
        )
        
        for ref in refs:
            ref_num = int(ref)
            if ref_num != chapter_num and ref_num <= len(chapters):
                dependencies[chapter_num]['requires'].append(ref_num)
                if ref_num in dependencies:
                    dependencies[ref_num]['referenced_by'].append(chapter_num)
    
    return dependencies

def detect_contradictions(chapters: List[Dict]) -> List[Dict]:
    """
    Detecta aparentes contradiÃ§Ãµes entre capÃ­tulos
    Modo BÃ¡sico: padrÃµes simples
    Modo Premium: anÃ¡lise semÃ¢ntica profunda
    """
    contradictions = []
    
    # PadrÃµes de linguagem contraditÃ³ria
    patterns = [
        (r'always\s+(\w+)', r'never\s+\1'),
        (r'should\s+(\w+)', r'should\s+not\s+\1'),
        (r'recommended', r'not\s+recommended')
    ]
    
    for i, chapter_a in enumerate(chapters):
        content_a = ' '.join(chapter_a['content']).lower()
        
        for j, chapter_b in enumerate(chapters[i+1:], start=i+1):
            content_b = ' '.join(chapter_b['content']).lower()
            
            for pos_pattern, neg_pattern in patterns:
                pos_matches = re.findall(pos_pattern, content_a)
                neg_matches = re.findall(neg_pattern, content_b)
                
                common = set(pos_matches) & set(neg_matches)
                if common:
                    contradictions.append({
                        'chapters': [i+1, j+1],
                        'topic': list(common)[0],
                        'type': 'apparent_contradiction'
                    })
    
    return contradictions

def build_semantic_index(concepts: Dict, chapters: List[Dict]) -> Dict:
    """
    Cria Ã­ndice semÃ¢ntico com relaÃ§Ãµes entre conceitos
    0 tokens - AnÃ¡lise de co-ocorrÃªncia
    """
    semantic_index = {}
    
    for concept, data in concepts.items():
        related = []
        concept_chapters = set(data['chapters'])
        
        # Encontrar conceitos que aparecem nos mesmos capÃ­tulos
        for other_concept, other_data in concepts.items():
            if other_concept == concept:
                continue
            
            other_chapters = set(other_data['chapters'])
            overlap = len(concept_chapters & other_chapters)
            
            if overlap >= 2:  # Aparecem juntos em 2+ capÃ­tulos
                related.append({
                    'concept': other_concept,
                    'strength': overlap
                })
        
        # Ordenar por forÃ§a de relaÃ§Ã£o
        related.sort(key=lambda x: x['strength'], reverse=True)
        
        semantic_index[concept] = {
            'appears_in': sorted(data['chapters']),
            'related_concepts': [r['concept'] for r in related[:10]],  # Top 10
            'type': data['type'],
            'frequency': data['frequency']
        }
    
    return semantic_index
```

### FASE 3: GERAÃ‡ÃƒO DE CONTEÃšDO (0 tokens - BÃ¡sico | VariÃ¡vel - Premium)

```python
# modules/content_generation.py

def generate_tldr_basic(chapter: Dict) -> str:
    """
    Modo BÃ¡sico: TL;DR por extraÃ§Ã£o automÃ¡tica
    0 tokens - Primeiras 2-3 frases do capÃ­tulo
    """
    content = chapter['content']
    if not content:
        return "No content available."
    
    # Pegar primeiras frases (atÃ© 200 chars)
    text = ' '.join(content[:3])
    sentences = re.split(r'[.!?]+', text)
    
    tldr = []
    char_count = 0
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        if char_count + len(sentence) > 200:
            break
        
        tldr.append(sentence)
        char_count += len(sentence)
    
    return '. '.join(tldr) + '.'

async def generate_tldr_premium(chapter: Dict, context: Dict) -> str:
    """
    Modo Premium: TL;DR gerado por IA
    ~36% do custo - Mais preciso e relevante
    """
    # Exemplo de prompt para API
    prompt = f"""
Crie um TL;DR conciso (mÃ¡ximo 100 tokens) deste capÃ­tulo:

TÃ­tulo: {chapter['title']}
ConteÃºdo: {' '.join(chapter['content'][:500])}...

O TL;DR deve:
- Capturar a essÃªncia do capÃ­tulo
- Ser acionÃ¡vel e claro
- Focar no mais relevante
- Usar linguagem objetiva

TL;DR:"""
    
    # Chamada Ã  API (exemplo)
    # response = await anthropic_client.messages.create(...)
    # return response.content[0].text
    
    pass

def create_faqs_basic(chapter: Dict) -> List[Dict]:
    """
    Modo BÃ¡sico: FAQs baseadas em tÃ­tulos de seÃ§Ãµes
    0 tokens - TransformaÃ§Ã£o automÃ¡tica
    """
    faqs = []
    
    # FAQ do tÃ­tulo do capÃ­tulo
    faqs.append({
        'question': f"What is {chapter['title']} about?",
        'answer': f"See the chapter summary above for an overview of {chapter['title']}."
    })
    
    # FAQ de cada seÃ§Ã£o
    for section in chapter.get('sections', [])[:5]:  # Top 5 seÃ§Ãµes
        # Transformar tÃ­tulo em pergunta
        title = section['title']
        
        # HeurÃ­sticas simples
        if title.startswith('HOW'):
            question = title + '?'
        elif title.startswith('WHAT'):
            question = title + '?'
        else:
            question = f"What about {title.lower()}?"
        
        # Resposta: primeiras 2 frases da seÃ§Ã£o
        answer_text = ' '.join(section['content'][:2])
        
        faqs.append({
            'question': question,
            'answer': answer_text
        })
    
    return faqs

async def create_faqs_premium(chapter: Dict, context: Dict) -> List[Dict]:
    """
    Modo Premium: FAQs inteligentes geradas por IA
    ~27% do custo - Perguntas realmente Ãºteis
    """
    # Exemplo de prompt
    prompt = f"""
Crie 5 FAQs relevantes e prÃ¡ticas para este capÃ­tulo:

TÃ­tulo: {chapter['title']}
ConteÃºdo: {' '.join(chapter['content'][:800])}...

Contexto do usuÃ¡rio: {context['business']}
Casos de uso: {context['use_cases']}

Gere FAQs que:
- Sejam perguntas que um profissional realmente faria
- Tenham respostas prÃ¡ticas e acionÃ¡veis
- Conectem teoria com aplicaÃ§Ã£o
- Sejam relevantes para o contexto do usuÃ¡rio

Formato JSON:
[
  {{"question": "...", "answer": "..."}},
  ...
]
"""
    pass

def extract_frameworks(chapter: Dict) -> List[Dict]:
    """
    Extrai frameworks, processos e checklists acionÃ¡veis
    0 tokens - DetecÃ§Ã£o por padrÃµes
    """
    frameworks = []
    content = '\n'.join(chapter['content'])
    
    # Detectar listas numeradas (processos)
    numbered_lists = re.findall(
        r'(?:^|\n)((?:\d+\.\s+.+\n?)+)',
        content,
        re.MULTILINE
    )
    
    for i, lst in enumerate(numbered_lists):
        steps = re.findall(r'\d+\.\s+(.+)', lst)
        if len(steps) >= 3:  # MÃ­nimo 3 passos
            frameworks.append({
                'type': 'process',
                'name': f"Process from {chapter['title']}",
                'steps': steps
            })
    
    # Detectar checklists (bullet points)
    bullet_lists = re.findall(
        r'(?:^|\n)((?:[-â€¢*]\s+.+\n?)+)',
        content,
        re.MULTILINE
    )
    
    for lst in bullet_lists:
        items = re.findall(r'[-â€¢*]\s+(.+)', lst)
        if len(items) >= 3:
            frameworks.append({
                'type': 'checklist',
                'name': f"Checklist from {chapter['title']}",
                'items': items
            })
    
    return frameworks

async def contextualize_examples(examples: List[str], context: Dict) -> List[Dict]:
    """
    Modo Premium: Adapta exemplos ao contexto do usuÃ¡rio
    ~23% do custo
    """
    # Exemplo de adaptaÃ§Ã£o com IA
    pass
```

### FASE 4: OTIMIZAÃ‡ÃƒO DE TOKENS (0 tokens)

```python
# modules/optimization.py

def create_layered_content(chapter: Dict, tldr: str, summary: str) -> Dict:
    """
    Cria estrutura em camadas: TL;DR â†’ Resumo â†’ Completo
    0 tokens - OrganizaÃ§Ã£o estrutural
    """
    full_content = '\n\n'.join(chapter['content'])
    
    # Estimar tokens (rough: 4 chars = 1 token)
    tldr_tokens = len(tldr) // 4
    summary_tokens = len(summary) // 4
    full_tokens = len(full_content) // 4
    
    return {
        'tldr': {
            'content': tldr,
            'tokens': tldr_tokens,
            'use_case': 'Quick reference, 60% of queries'
        },
        'summary': {
            'content': summary,
            'tokens': summary_tokens,
            'use_case': 'Standard queries, 25% of queries'
        },
        'full': {
            'content': full_content,
            'tokens': full_tokens,
            'use_case': 'Deep dive, 15% of queries'
        }
    }

def split_large_chapters(chapter: Dict, max_tokens: int = 15000) -> List[Dict]:
    """
    Quebra capÃ­tulos grandes em mÃºltiplos arquivos
    0 tokens - DivisÃ£o por seÃ§Ãµes
    """
    chapter_tokens = sum(len(c) for c in chapter['content']) // 4
    
    if chapter_tokens <= max_tokens:
        return [chapter]
    
    # Dividir por seÃ§Ãµes
    parts = []
    current_part = {
        'title': f"{chapter['title']} - Part 1",
        'number': chapter['number'],
        'part': 1,
        'sections': [],
        'content': []
    }
    current_tokens = 0
    
    for section in chapter.get('sections', []):
        section_tokens = sum(len(c) for c in section['content']) // 4
        
        if current_tokens + section_tokens > max_tokens and current_part['sections']:
            parts.append(current_part)
            current_part = {
                'title': f"{chapter['title']} - Part {len(parts) + 1}",
                'number': chapter['number'],
                'part': len(parts) + 1,
                'sections': [],
                'content': []
            }
            current_tokens = 0
        
        current_part['sections'].append(section)
        current_tokens += section_tokens
    
    if current_part['sections']:
        parts.append(current_part)
    
    return parts if len(parts) > 1 else [chapter]

def build_navigation_map(chapters: List[Dict], dependencies: Dict) -> str:
    """
    Cria mapa de navegaÃ§Ã£o inteligente em Mermaid
    0 tokens - GeraÃ§Ã£o de diagrama
    """
    mermaid = ["```mermaid", "graph TD"]
    
    # Agrupar capÃ­tulos por dependÃªncia
    for i, chapter in enumerate(chapters):
        node_id = f"C{i+1}"
        title = chapter['title'][:30] + "..." if len(chapter['title']) > 30 else chapter['title']
        mermaid.append(f"    {node_id}[\"{title}\"]")
    
    # Adicionar dependÃªncias
    for chapter_num, deps in dependencies.items():
        for req in deps['requires']:
            mermaid.append(f"    C{req} --> C{chapter_num}")
    
    mermaid.append("```")
    return '\n'.join(mermaid)

def generate_applicability_matrix(chapters: List[Dict], contexts: List[str]) -> str:
    """
    Gera matriz de aplicabilidade por contexto
    0 tokens - AnÃ¡lise de keywords
    """
    # Contexts tÃ­picos
    default_contexts = [
        'B2B', 'B2C', 'SaaS', 'Product', 'Startup', 'Enterprise'
    ]
    
    if not contexts:
        contexts = default_contexts
    
    # Criar tabela markdown
    header = "| Chapter | " + " | ".join(contexts) + " |"
    separator = "|---------|" + "|".join(["-----" for _ in contexts]) + "|"
    
    rows = [header, separator]
    
    for chapter in chapters:
        content = ' '.join(chapter['content']).lower()
        row = [f"Cap {chapter['number']}"]
        
        for context in contexts:
            # AnÃ¡lise simples de presenÃ§a de keywords
            context_score = content.count(context.lower())
            
            if context_score > 10:
                row.append("âœ“âœ“âœ“")
            elif context_score > 5:
                row.append("âœ“âœ“")
            elif context_score > 0:
                row.append("âœ“")
            else:
                row.append("âœ—")
        
        rows.append("| " + " | ".join(row) + " |")
    
    return '\n'.join(rows)
```

### FASE 5: GERAÃ‡ÃƒO DE ARTEFATOS (0 tokens)

```python
# modules/artifacts.py

def create_presentation(book_data: Dict, chapters: List[Dict]) -> str:
    """
    Gera apresentaÃ§Ã£o executiva em Markdown
    0 tokens - Template + dados
    """
    slides = [
        f"# {book_data['metadata']['title']}",
        f"## Executive Summary",
        "",
        f"**Author**: {book_data['metadata']['author']}",
        f"**Pages**: {book_data['metadata']['pages']}",
        "",
        "---",
        "",
        "## Key Topics",
        ""
    ]
    
    # Slide por capÃ­tulo (principais)
    for chapter in chapters[:5]:  # Top 5 capÃ­tulos
        slides.extend([
            f"## {chapter['title']}",
            "",
            f"**Key Concepts**:",
            ""
        ])
        
        # Extrair primeiros conceitos
        for section in chapter.get('sections', [])[:3]:
            slides.append(f"- {section['title']}")
        
        slides.extend(["", "---", ""])
    
    return '\n'.join(slides)

def build_implementation_plan(frameworks: List[Dict], context: Dict) -> str:
    """
    Cria plano de implementaÃ§Ã£o de 90 dias
    0 tokens - Template estruturado
    """
    plan = [
        "# Implementation Plan - 90 Days",
        "",
        f"**Context**: {context.get('business', 'General')}",
        "",
        "## Phase 1: Weeks 1-4 (Foundation)",
        ""
    ]
    
    # Distribuir frameworks nas fases
    phase1 = frameworks[:len(frameworks)//3]
    phase2 = frameworks[len(frameworks)//3:2*len(frameworks)//3]
    phase3 = frameworks[2*len(frameworks)//3:]
    
    for fw in phase1:
        plan.append(f"### {fw['name']}")
        plan.append("")
        if fw['type'] == 'process':
            for step in fw['steps']:
                plan.append(f"- [ ] {step}")
        plan.append("")
    
    plan.extend([
        "## Phase 2: Weeks 5-8 (Development)",
        ""
    ])
    
    for fw in phase2:
        plan.append(f"### {fw['name']}")
        plan.append("")
        if fw['type'] == 'checklist':
            for item in fw['items']:
                plan.append(f"- [ ] {item}")
        plan.append("")
    
    plan.extend([
        "## Phase 3: Weeks 9-12 (Optimization)",
        ""
    ])
    
    for fw in phase3:
        plan.append(f"### {fw['name']}")
        plan.append("")
    
    return '\n'.join(plan)

def generate_one_pager(book_data: Dict, summary: str, key_concepts: List[str]) -> str:
    """
    Cria one-pager executivo
    0 tokens - SÃ­ntese visual
    """
    one_pager = [
        f"# {book_data['metadata']['title']} - One Pager",
        "",
        "## ğŸ“š Overview",
        "",
        summary[:500] + "...",
        "",
        "## ğŸ¯ Key Concepts",
        ""
    ]
    
    for concept in key_concepts[:10]:  # Top 10
        one_pager.append(f"- **{concept}**")
    
    one_pager.extend([
        "",
        "## ğŸš€ Quick Start",
        "",
        "1. Read chapters 1-3 for foundation",
        "2. Focus on chapters relevant to your use case",
        "3. Use frameworks and templates provided",
        "",
        "## ğŸ“Š Application Matrix",
        "",
        "See `aplicabilidade-matrix.md` for detailed breakdown"
    ])
    
    return '\n'.join(one_pager)
```

### FASE 6: ASSEMBLY FINAL (0 tokens)

```python
# modules/skill_assembly.py

import json
from pathlib import Path

def create_skill_structure(output_dir: str, book_title: str):
    """
    Cria estrutura de pastas da skill
    0 tokens - I/O de arquivos
    """
    base_path = Path(output_dir)
    
    # Estrutura de diretÃ³rios
    dirs = [
        base_path,
        base_path / "capitulos",
        base_path / "frameworks",
        base_path / "exemplos",
        base_path / "artifacts",
        base_path / "indices"
    ]
    
    for dir_path in dirs:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    return base_path

def generate_main_skill_md(
    book_data: Dict,
    chapters: List[Dict],
    concepts: Dict,
    dependencies: Dict,
    context: Dict,
    mode: str
) -> str:
    """
    Gera o SKILL.md principal da skill especializada
    0 tokens - Template preenchido
    """
    
    # Calcular estatÃ­sticas
    num_concepts = len(concepts)
    num_chapters = len(chapters)
    total_pages = book_data['metadata']['pages']
    
    # Criar lista de capÃ­tulos
    chapter_list = []
    for ch in chapters:
        chapter_list.append(f"- **Chapter {ch['number']}**: {ch['title']}")
    
    # Criar Ã­ndice de conceitos principais
    top_concepts = sorted(concepts.items(), key=lambda x: x[1]['frequency'], reverse=True)[:20]
    concept_index = []
    for concept, data in top_concepts:
        chapters_str = ', '.join([f"Cap {c}" for c in data['chapters']])
        concept_index.append(f"- **{concept}**: {chapters_str}")
    
    skill_md = f"""# {book_data['metadata']['title']} - Expert Skill

## ğŸ“š Sobre Este Livro

**Autor**: {book_data['metadata']['author']}  
**PÃ¡ginas**: {total_pages}  
**CapÃ­tulos**: {num_chapters}  
**Conceitos identificados**: {num_concepts}  
**Modo de conversÃ£o**: {"Premium (Ultra-Otimizado)" if mode == "premium" else "BÃ¡sico (Estrutural)"}

Esta skill transforma Claude em um **especialista virtual** neste livro, capaz de:
- âœ… Responder perguntas com base no conteÃºdo completo
- âœ… Aplicar conceitos ao contexto especÃ­fico do usuÃ¡rio
- âœ… Criar implementaÃ§Ãµes prÃ¡ticas e personalizadas
- âœ… Conectar ideias de diferentes capÃ­tulos
- âœ… Adaptar exemplos ao seu negÃ³cio

## ğŸ¯ Quando Claude Deve Usar Esta Skill

**Sempre que o usuÃ¡rio**:
- Mencionar "{book_data['metadata']['title']}"
- Perguntar sobre tÃ³picos relacionados aos conceitos do livro
- Solicitar aplicaÃ§Ã£o prÃ¡tica dos frameworks
- Pedir implementaÃ§Ãµes baseadas no mÃ©todo do livro
- Referenciar capÃ­tulos ou conceitos especÃ­ficos

**Contexto de personalizaÃ§Ã£o**: {context.get('business', 'Geral')}  
**Casos de uso**: {context.get('use_cases', 'Diversos')}  
**Exemplos adaptados para**: {context.get('adapt_for', 'Contexto geral')}

## ğŸ§­ Sistema de NavegaÃ§Ã£o Adaptativa

Claude deve usar navegaÃ§Ã£o inteligente baseada no tipo de pergunta:

### NÃ­veis de Profundidade

**ğŸ“Œ NÃ­vel 1 - RÃ¡pido** (~2-5k tokens):
- **Quando usar**: Perguntas simples, definiÃ§Ãµes, conceitos gerais
- **O que ler**: SKILL.md + TL;DR do capÃ­tulo relevante + FAQ
- **Exemplo**: "O que Ã© [conceito]?"
- **Token mÃ©dio**: 3k

**ğŸ“– NÃ­vel 2 - PadrÃ£o** (~15-20k tokens):
- **Quando usar**: Perguntas sobre aplicaÃ§Ã£o, how-to, explicaÃ§Ãµes
- **O que ler**: SKILL.md + Resumo + CapÃ­tulo completo
- **Exemplo**: "Como aplicar [framework]?"
- **Token mÃ©dio**: 18k

**ğŸ”¬ NÃ­vel 3 - Profundo** (~40-60k tokens):
- **Quando usar**: AnÃ¡lises, comparaÃ§Ãµes, mÃºltiplos conceitos
- **O que ler**: SKILL.md + MÃºltiplos capÃ­tulos + ReferÃªncias cruzadas
- **Exemplo**: "Compare os mÃ©todos X, Y e Z"
- **Token mÃ©dio**: 50k

**ğŸ“ NÃ­vel 4 - Pesquisa** (80k+ tokens):
- **Quando usar**: CriaÃ§Ã£o de estratÃ©gias, planos completos, sÃ­nteses
- **O que ler**: Conhecimento abrangente + Frameworks + Exemplos
- **Exemplo**: "Crie estratÃ©gia completa usando o livro"
- **Token mÃ©dio**: 100k+

### Auto-DetecÃ§Ã£o de NÃ­vel

Claude detecta automaticamente o nÃ­vel necessÃ¡rio:

```python
if pergunta contÃ©m ["o que Ã©", "defin", "conceito"]:
    nivel = 1  # TL;DR + FAQ
elif pergunta contÃ©m ["como", "aplicar", "implementar", "usar"]:
    nivel = 2  # Resumo + CapÃ­tulo
elif pergunta contÃ©m ["comparar", "diferenÃ§a", "versus", "anÃ¡lise"]:
    nivel = 3  # MÃºltiplos capÃ­tulos
elif pergunta contÃ©m ["criar", "estratÃ©gia", "plano", "framework completo"]:
    nivel = 4  # Conhecimento abrangente
else:
    nivel = 2  # PadrÃ£o
```

## ğŸ“– Estrutura do Conhecimento

### Arquivos DisponÃ­veis

```
/revenue-architecture/
â”œâ”€â”€ SKILL.md (este arquivo - navegaÃ§Ã£o geral)
â”œâ”€â”€ resumo-executivo.md (visÃ£o geral do livro)
â”œâ”€â”€ glossario.md (todos os termos definidos)
â”œâ”€â”€ conhecimento-map.md (Ã¡rvore de dependÃªncias)
â”œâ”€â”€ aplicabilidade-matrix.md (por contexto de negÃ³cio)
â”œâ”€â”€ nuances.md (contradiÃ§Ãµes resolvidas)
â”œâ”€â”€ metadata.json (Ã­ndice semÃ¢ntico completo)
â”œâ”€â”€ changelog.md (histÃ³rico de versÃµes)
â”‚
â”œâ”€â”€ capitulos/
â”‚   â”œâ”€â”€ capitulo-01.md
â”‚   â”œâ”€â”€ faq-capitulo-01.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ frameworks/
â”‚   â”œâ”€â”€ [frameworks extraÃ­dos]
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ exemplos/
â”‚   â”œâ”€â”€ contextualizados/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ apresentacao-executiva.md
â”‚   â”œâ”€â”€ plano-implementacao.md
â”‚   â””â”€â”€ one-pager.md
â”‚
â””â”€â”€ indices/
    â”œâ”€â”€ por-tema.md
    â”œâ”€â”€ por-palavra-chave.md
    â””â”€â”€ por-aplicacao.md
```

### Mapa de Conhecimento

Ver arquivo `conhecimento-map.md` para Ã¡rvore completa de dependÃªncias.

**SequÃªncia recomendada de aprendizado**:
1. Fundamentos (Cap 1-3)
2. Metodologia Core (Cap 4-6)
3. AplicaÃ§Ã£o PrÃ¡tica (Cap 7-9)
4. Casos AvanÃ§ados (Cap 10-12)

## ğŸ“‘ Ãndice Completo de CapÃ­tulos

{chr(10).join(chapter_list)}

## ğŸ” Ãndice de Conceitos Principais

{chr(10).join(concept_index)}

Ver `glossario.md` para lista completa com definiÃ§Ãµes.

## ğŸ¯ Como Responder Perguntas

### Para Conceitos e DefiniÃ§Ãµes
1. Consultar `glossario.md` primeiro
2. Se nÃ£o encontrar, buscar no TL;DR dos capÃ­tulos relevantes
3. Sempre citar capÃ­tulo e seÃ§Ã£o de origem

### Para AplicaÃ§Ãµes PrÃ¡ticas
1. Identificar capÃ­tulo relevante via Ã­ndice temÃ¡tico
2. Ler resumo do capÃ­tulo primeiro
3. Se necessÃ¡rio, ler capÃ­tulo completo
4. Consultar `frameworks/` para templates prontos
5. Adaptar ao contexto: {context.get('business', 'geral')}
6. Oferecer exemplos de `exemplos/contextualizados/`

### Para ComparaÃ§Ãµes
1. Identificar capÃ­tulos que tratam dos tÃ³picos
2. Consultar `nuances.md` para contradiÃ§Ãµes conhecidas
3. Ler resumos de todos os capÃ­tulos envolvidos
4. Criar comparaÃ§Ã£o estruturada
5. Citar seÃ§Ãµes especÃ­ficas

### Para ImplementaÃ§Ãµes
1. Consultar `frameworks/` para templates
2. Ler capÃ­tulos relevantes (completos)
3. Usar `plano-implementacao.md` como base
4. Personalizar para contexto do usuÃ¡rio
5. Criar checklists acionÃ¡veis

## ğŸ¨ InstruÃ§Ãµes Especiais

### Sempre:
- âœ… Citar capÃ­tulo e seÃ§Ã£o ao responder
- âœ… Adaptar linguagem e exemplos ao contexto: {context.get('business', 'geral')}
- âœ… Oferecer frameworks/templates quando aplicÃ¡vel
- âœ… Conectar conceitos de diferentes capÃ­tulos quando relevante
- âœ… Sugerir leituras complementares de outros capÃ­tulos
- âœ… Usar camada apropriada (TL;DR vs Resumo vs Completo)

### Nunca:
- âŒ Inventar informaÃ§Ãµes que nÃ£o estÃ£o no livro
- âŒ Citar capÃ­tulos sem ler o conteÃºdo
- âŒ Ignorar o contexto de personalizaÃ§Ã£o
- âŒ Carregar mais conteÃºdo que o necessÃ¡rio
- âŒ Misturar informaÃ§Ãµes de fontes externas ao livro

### Formato de CitaÃ§Ã£o

Sempre usar este formato:
```
[Conceito explicado] (Cap X, SeÃ§Ã£o Y)
```

Exemplo:
```
O framework de Revenue Architecture se baseia em trÃªs pilares
fundamentais: pessoas, processos e tecnologia (Cap 3, SeÃ§Ã£o 3.2).
```

## ğŸ’¡ Dicas de Uso para o UsuÃ¡rio

### Perguntas Eficazes

**âœ… Bom**: "Como aplicar o framework do CapÃ­tulo 5 na minha empresa?"  
**âŒ Ruim**: "Me explica tudo sobre revenue"

**âœ… Bom**: "Qual a diferenÃ§a entre os mÃ©todos X e Y?"  
**âŒ Ruim**: "O que o livro fala?"

**âœ… Bom**: "Crie um plano de 30 dias para implementar [conceito]"  
**âŒ Ruim**: "Resuma o livro"

### Maximizando Valor

1. **Seja especÃ­fico**: Mencione capÃ­tulos ou conceitos quando souber
2. **ForneÃ§a contexto**: Quanto mais contexto sobre sua situaÃ§Ã£o, melhor a adaptaÃ§Ã£o
3. **PeÃ§a artefatos**: "Crie uma apresentaÃ§Ã£o", "Gere um checklist"
4. **Explore conexÃµes**: "Como isso se relaciona com [outro conceito]?"
5. **Itere**: FaÃ§a perguntas de follow-up para aprofundar

## ğŸ“Š EstatÃ­sticas da Skill

- **Modo de conversÃ£o**: {mode.title()}
- **Tokens de geraÃ§Ã£o**: {"~110k (investimento em qualidade)" if mode == "premium" else "0 (estrutural puro)"}
- **Token mÃ©dio por consulta**: {"~5.6k" if mode == "premium" else "~15k"}
- **Economia vs sem skill**: {"~88%" if mode == "premium" else "~73%"}
- **CapÃ­tulos processados**: {num_chapters}
- **Conceitos identificados**: {num_concepts}
- **Frameworks extraÃ­dos**: [calculado na geraÃ§Ã£o]
- **FAQs geradas**: [calculado na geraÃ§Ã£o]

## ğŸ”„ Versionamento

Ver `changelog.md` para histÃ³rico completo.

**VersÃ£o atual**: 1.0.0  
**Data de criaÃ§Ã£o**: [DATA]  
**Ãšltima atualizaÃ§Ã£o**: [DATA]

## ğŸ†˜ Troubleshooting

**P: Claude nÃ£o estÃ¡ encontrando informaÃ§Ã£o que sei que existe**  
R: Especifique o capÃ­tulo se souber. Use o Ã­ndice de conceitos.

**P: Resposta muito genÃ©rica, quero mais detalhe**  
R: PeÃ§a explicitamente: "Me dÃª a explicaÃ§Ã£o completa do Cap X"

**P: Quero adaptar para meu contexto especÃ­fico**  
R: ForneÃ§a mais detalhes sobre seu contexto na pergunta

**P: Como posso ver os frameworks disponÃ­veis?**  
R: PeÃ§a: "Liste todos os frameworks extraÃ­dos do livro"

---

**Esta skill foi gerada automaticamente pela pdf-to-skill-converter**  
**Para reportar problemas ou sugerir melhorias, atualize a skill base**
"""
    
    return skill_md

def create_chapter_file(chapter: Dict, tldr: str, summary: str, faqs: List[Dict], frameworks: List[Dict]) -> str:
    """
    Cria arquivo markdown de um capÃ­tulo
    0 tokens - Template preenchido
    """
    # Estimar tokens
    full_content = '\n\n'.join(chapter['content'])
    tldr_tokens = len(tldr) // 4
    summary_tokens = len(summary) // 4
    full_tokens = len(full_content) // 4
    
    # Extrair tags (conceitos mencionados)
    tags = []
    # ImplementaÃ§Ã£o simplificada
    
    chapter_md = f"""---
capitulo: {chapter['number']}
titulo: "{chapter['title']}"
tags: {', '.join(tags) if tags else 'N/A'}
tokens_tldr: {tldr_tokens}
tokens_resumo: {summary_tokens}
tokens_completo: {full_tokens}
---

# CapÃ­tulo {chapter['number']}: {chapter['title']}

## ğŸ“Œ TL;DR (~{tldr_tokens} tokens)

{tldr}

---

## ğŸ“– Resumo Executivo (~{summary_tokens} tokens)

{summary}

---

## ğŸ¯ Conceitos-Chave

"""
    
    # Adicionar conceitos extraÃ­dos
    for section in chapter.get('sections', [])[:5]:
        chapter_md += f"- **{section['title']}**\n"
    
    chapter_md += f"""
---

## ğŸ“š ConteÃºdo Completo (~{full_tokens} tokens)

{full_content}

---

## ğŸš€ Frameworks ExtraÃ­dos

"""
    
    if frameworks:
        for fw in frameworks:
            chapter_md += f"\n### {fw['name']}\n\n"
            if fw['type'] == 'process':
                for i, step in enumerate(fw['steps'], 1):
                    chapter_md += f"{i}. {step}\n"
            elif fw['type'] == 'checklist':
                for item in fw['items']:
                    chapter_md += f"- [ ] {item}\n"
            chapter_md += "\n"
    else:
        chapter_md += "Nenhum framework identificado neste capÃ­tulo.\n"
    
    chapter_md += """
---

## â“ FAQ do CapÃ­tulo

"""
    
    for faq in faqs:
        chapter_md += f"\n### {faq['question']}\n\n{faq['answer']}\n"
    
    chapter_md += """
---

## ğŸ”— ReferÃªncias Cruzadas

Ver `conhecimento-map.md` para dependÃªncias deste capÃ­tulo.

---

## ğŸ“Š Aplicabilidade

Ver `aplicabilidade-matrix.md` para contextos onde este capÃ­tulo Ã© mais relevante.
"""
    
    return chapter_md

def create_metadata_json(
    book_data: Dict,
    chapters: List[Dict],
    concepts: Dict,
    dependencies: Dict,
    semantic_index: Dict
) -> str:
    """
    Cria metadata.json com Ã­ndice completo
    0 tokens - SerializaÃ§Ã£o JSON
    """
    metadata = {
        'book': {
            'title': book_data['metadata']['title'],
            'author': book_data['metadata']['author'],
            'pages': book_data['metadata']['pages'],
            'processed_date': '2025-10-27',  # Dynamic
            'version': '1.0.0'
        },
        'structure': {
            'num_chapters': len(chapters),
            'num_concepts': len(concepts),
            'chapters': [
                {
                    'number': ch['number'],
                    'title': ch['title'],
                    'sections': len(ch.get('sections', []))
                }
                for ch in chapters
            ]
        },
        'semantic_index': semantic_index,
        'dependencies': dependencies,
        'statistics': {
            'total_concepts': len(concepts),
            'total_frameworks': sum(len(ch.get('frameworks', [])) for ch in chapters),
            'avg_tokens_per_chapter': sum(len('\n'.join(ch['content'])) for ch in chapters) // len(chapters) // 4
        }
    }
    
    return json.dumps(metadata, indent=2, ensure_ascii=False)

def validate_output(skill_path: Path) -> Dict[str, bool]:
    """
    Valida estrutura gerada
    0 tokens - Checagem de arquivos
    """
    checks = {
        'SKILL.md exists': (skill_path / 'SKILL.md').exists(),
        'Chapters directory': (skill_path / 'capitulos').exists(),
        'Frameworks directory': (skill_path / 'frameworks').exists(),
        'Metadata exists': (skill_path / 'metadata.json').exists(),
        'Glossary exists': (skill_path / 'glossario.md').exists(),
    }
    
    # Count chapters
    chapters_dir = skill_path / 'capitulos'
    if chapters_dir.exists():
        num_chapters = len(list(chapters_dir.glob('capitulo-*.md')))
        checks[f'{num_chapters} chapters generated'] = num_chapters > 0
    
    return checks
```

### FASE 7: ORQUESTRAÃ‡ÃƒO COMPLETA

```python
# main_converter.py - Script principal

import sys
from pathlib import Path
from modules import extraction, semantic_analysis, content_generation
from modules import optimization, artifacts, skill_assembly

async def convert_pdf_to_skill(
    pdf_path: str,
    output_dir: str,
    mode: str = 'basico',
    context: Dict = None
):
    """
    Pipeline completo de conversÃ£o
    """
    
    print("ğŸš€ Iniciando conversÃ£o...")
    print(f"ğŸ“„ PDF: {pdf_path}")
    print(f"ğŸ“ Output: {output_dir}")
    print(f"âš™ï¸  Modo: {mode.title()}")
    print()
    
    # FASE 1: ExtraÃ§Ã£o
    print("ğŸ“– FASE 1: Extraindo PDF...")
    book_data = extraction.extract_pdf(pdf_path)
    print(f"   âœ… {book_data['num_pages']} pÃ¡ginas extraÃ­das (0 tokens)")
    
    print("ğŸ” Detectando estrutura...")
    chapters = extraction.detect_hierarchy(book_data)
    print(f"   âœ… {len(chapters)} capÃ­tulos identificados (0 tokens)")
    
    print("ğŸ¯ Extraindo elementos especiais...")
    chapters = extraction.extract_elements(chapters)
    print(f"   âœ… Tabelas, listas e cÃ³digo extraÃ­dos (0 tokens)")
    print()
    
    # FASE 2: AnÃ¡lise SemÃ¢ntica
    print("ğŸ§  FASE 2: AnÃ¡lise SemÃ¢ntica...")
    
    if mode == 'basico':
        print("   ğŸ“Š Modo BÃ¡sico: AnÃ¡lise estrutural...")
        concepts = semantic_analysis.identify_concepts_basic(chapters)
        print(f"   âœ… {len(concepts)} conceitos identificados (0 tokens)")
    else:
        print("   ğŸ¤– Modo Premium: AnÃ¡lise com IA...")
        concepts = await semantic_analysis.identify_concepts_premium(chapters, context)
        print(f"   âœ… {len(concepts)} conceitos identificados (~40k tokens)")
    
    print("   ğŸŒ³ Mapeando dependÃªncias...")
    dependencies = semantic_analysis.map_dependencies(chapters)
    print(f"   âœ… Ãrvore de conhecimento criada (0 tokens)")
    
    print("   ğŸ” Detectando contradiÃ§Ãµes...")
    contradictions = semantic_analysis.detect_contradictions(chapters)
    print(f"   âœ… {len(contradictions)} nuances identificadas (0 tokens)")
    
    print("   ğŸ”— Criando Ã­ndice semÃ¢ntico...")
    semantic_index = semantic_analysis.build_semantic_index(concepts, chapters)
    print(f"   âœ… Ãndice semÃ¢ntico gerado (0 tokens)")
    print()
    
    # FASE 3: GeraÃ§Ã£o de ConteÃºdo
    print("âœï¸  FASE 3: Gerando ConteÃºdo...")
    
    chapter_contents = []
    total_refinement_tokens = 0
    
    for i, chapter in enumerate(chapters, 1):
        print(f"   ğŸ“ Processando CapÃ­tulo {i}/{len(chapters)}: {chapter['title'][:40]}...")
        
        # TL;DR
        if mode == 'basico':
            tldr = content_generation.generate_tldr_basic(chapter)
            tldr_tokens = 0
        else:
            tldr = await content_generation.generate_tldr_premium(chapter, context)
            tldr_tokens = len(tldr) // 4
            total_refinement_tokens += tldr_tokens
        
        # Summary (sempre bÃ¡sico - primeira parte do capÃ­tulo)
        summary = ' '.join(chapter['content'][:5])
        
        # FAQs
        if mode == 'basico':
            faqs = content_generation.create_faqs_basic(chapter)
            faq_tokens = 0
        else:
            faqs = await content_generation.create_faqs_premium(chapter, context)
            faq_tokens = sum(len(f['question']) + len(f['answer']) for f in faqs) // 4
            total_refinement_tokens += faq_tokens
        
        # Frameworks
        frameworks = content_generation.extract_frameworks(chapter)
        
        chapter_contents.append({
            'chapter': chapter,
            'tldr': tldr,
            'summary': summary,
            'faqs': faqs,
            'frameworks': frameworks
        })
    
    if mode == 'premium':
        print(f"   ğŸ¤– Total de tokens de refinamento: ~{total_refinement_tokens}k")
    print()
    
    # FASE 4: OtimizaÃ§Ã£o
    print("âš¡ FASE 4: OtimizaÃ§Ã£o de Tokens...")
    
    print("   ğŸ“Š Criando camadas de conteÃºdo...")
    for cc in chapter_contents:
        cc['layers'] = optimization.create_layered_content(
            cc['chapter'], cc['tldr'], cc['summary']
        )
    print("   âœ… Estrutura em camadas criada (0 tokens)")
    
    print("   âœ‚ï¸  Dividindo capÃ­tulos grandes...")
    split_chapters = []
    for cc in chapter_contents:
        parts = optimization.split_large_chapters(cc['chapter'])
        if len(parts) > 1:
            print(f"      ğŸ“„ Cap {cc['chapter']['number']} dividido em {len(parts)} partes")
        split_chapters.extend(parts)
    print("   âœ… CapÃ­tulos grandes otimizados (0 tokens)")
    
    print("   ğŸ—ºï¸  Gerando mapa de navegaÃ§Ã£o...")
    nav_map = optimization.build_navigation_map(chapters, dependencies)
    print("   âœ… Mapa de navegaÃ§Ã£o criado (0 tokens)")
    
    print("   ğŸ“‹ Criando matriz de aplicabilidade...")
    contexts = [context.get('business', '')] if context else []
    app_matrix = optimization.generate_applicability_matrix(chapters, contexts)
    print("   âœ… Matriz de aplicabilidade gerada (0 tokens)")
    print()
    
    # FASE 5: Artefatos
    print("ğŸ¨ FASE 5: Gerando Artefatos...")
    
    print("   ğŸ“Š Criando apresentaÃ§Ã£o executiva...")
    presentation = artifacts.create_presentation(book_data, chapters)
    print("   âœ… ApresentaÃ§Ã£o gerada (0 tokens)")
    
    print("   ğŸ“… Gerando plano de implementaÃ§Ã£o...")
    all_frameworks = []
    for cc in chapter_contents:
        all_frameworks.extend(cc['frameworks'])
    impl_plan = artifacts.build_implementation_plan(all_frameworks, context or {})
    print("   âœ… Plano de implementaÃ§Ã£o criado (0 tokens)")
    
    print("   ğŸ“„ Criando one-pager...")
    summary_text = ' '.join(chapters[0]['content'][:10])
    top_concepts = list(concepts.keys())[:10]
    one_pager = artifacts.generate_one_pager(book_data, summary_text, top_concepts)
    print("   âœ… One-pager gerado (0 tokens)")
    print()
    
    # FASE 6: Assembly
    print("ğŸ—ï¸  FASE 6: Montando Skill...")
    
    print("   ğŸ“ Criando estrutura de diretÃ³rios...")
    skill_path = skill_assembly.create_skill_structure(
        output_dir,
        book_data['metadata']['title']
    )
    print(f"   âœ… Estrutura criada em {skill_path}")
    
    print("   ğŸ“ Gerando SKILL.md principal...")
    main_skill = skill_assembly.generate_main_skill_md(
        book_data, chapters, concepts, dependencies,
        context or {}, mode
    )
    (skill_path / 'SKILL.md').write_text(main_skill, encoding='utf-8')
    print("   âœ… SKILL.md criado")
    
    print("   ğŸ“š Gerando arquivos de capÃ­tulos...")
    capitulos_dir = skill_path / 'capitulos'
    for cc in chapter_contents:
        chapter_file = skill_assembly.create_chapter_file(
            cc['chapter'], cc['tldr'], cc['summary'],
            cc['faqs'], cc['frameworks']
        )
        filename = f"capitulo-{cc['chapter']['number']:02d}.md"
        (capitulos_dir / filename).write_text(chapter_file, encoding='utf-8')
    print(f"   âœ… {len(chapter_contents)} capÃ­tulos gerados")
    
    print("   ğŸ—‚ï¸  Salvando artefatos...")
    artifacts_dir = skill_path / 'artifacts'
    (artifacts_dir / 'apresentacao-executiva.md').write_text(presentation, encoding='utf-8')
    (artifacts_dir / 'plano-implementacao.md').write_text(impl_plan, encoding='utf-8')
    (artifacts_dir / 'one-pager.md').write_text(one_pager, encoding='utf-8')
    print("   âœ… Artefatos salvos")
    
    print("   ğŸ“Š Gerando metadata.json...")
    metadata_json = skill_assembly.create_metadata_json(
        book_data, chapters, concepts, dependencies, semantic_index
    )
    (skill_path / 'metadata.json').write_text(metadata_json, encoding='utf-8')
    print("   âœ… Metadata gerado")
    
    print("   ğŸ—ºï¸  Salvando auxiliares...")
    (skill_path / 'conhecimento-map.md').write_text(nav_map, encoding='utf-8')
    (skill_path / 'aplicabilidade-matrix.md').write_text(app_matrix, encoding='utf-8')
    print("   âœ… Arquivos auxiliares salvos")
    print()
    
    # FASE 7: ValidaÃ§Ã£o
    print("âœ… FASE 7: ValidaÃ§Ã£o...")
    validation = skill_assembly.validate_output(skill_path)
    
    all_passed = True
    for check, passed in validation.items():
        status = "âœ…" if passed else "âŒ"
        print(f"   {status} {check}")
        if not passed:
            all_passed = False
    print()
    
    # RelatÃ³rio Final
    print("=" * 60)
    print("ğŸ‰ CONVERSÃƒO CONCLUÃDA!")
    print("=" * 60)
    print()
    print(f"ğŸ“Š EstatÃ­sticas:")
    print(f"   â€¢ PÃ¡ginas processadas: {book_data['num_pages']}")
    print(f"   â€¢ CapÃ­tulos gerados: {len(chapter_contents)}")
    print(f"   â€¢ Conceitos identificados: {len(concepts)}")
    print(f"   â€¢ Frameworks extraÃ­dos: {len(all_frameworks)}")
    print(f"   â€¢ FAQs geradas: {sum(len(cc['faqs']) for cc in chapter_contents)}")
    print()
    print(f"ğŸ’° Custo de Tokens:")
    if mode == 'basico':
        print(f"   â€¢ GeraÃ§Ã£o: 0 tokens (100% local)")
        print(f"   â€¢ Custo: $0.00")
    else:
        print(f"   â€¢ GeraÃ§Ã£o: ~{total_refinement_tokens}k tokens")
        cost = (total_refinement_tokens * 1000 / 1_000_000) * 3
        print(f"   â€¢ Custo: ~${cost:.2f}")
        print(f"   â€¢ Break-even: ~{int(cost / 0.148)} consultas")
    print()
    print(f"ğŸ“ Output:")
    print(f"   â€¢ LocalizaÃ§Ã£o: {skill_path}")
    print(f"   â€¢ Arquivos gerados: {sum(1 for _ in skill_path.rglob('*') if _.is_file())}")
    print()
    print(f"ğŸš€ PrÃ³ximos Passos:")
    print(f"   1. Revisar resumo-executivo.md")
    print(f"   2. Validar exemplos contextualizados (se Premium)")
    print(f"   3. Mover para /mnt/skills/user/ para ativaÃ§Ã£o")
    print(f"   4. Testar com perguntas prÃ¡ticas")
    print()
    print(f"ğŸ’¡ Uso:")
    print(f"   Agora vocÃª pode fazer perguntas como:")
    print(f'   â€¢ "Como aplicar [conceito] na {context.get("adapt_for", "minha empresa")}?"')
    print(f'   â€¢ "Compare os mÃ©todos X e Y do livro"')
    print(f'   â€¢ "Crie um plano de implementaÃ§Ã£o baseado no CapÃ­tulo 5"')
    print()
    
    return skill_path

# Entry point
if __name__ == "__main__":
    # Este script seria executado pelo Claude
    pass
```

## ğŸ“‹ InstruÃ§Ãµes para Claude

### Quando o UsuÃ¡rio Solicitar ConversÃ£o

1. **LER ESTE SKILL.MD COMPLETAMENTE**

2. **APRESENTAR OPÃ‡Ã•ES** usando o template da Fase 0
   - Calcular custos dinamicamente baseado no nÃºmero de pÃ¡ginas
   - Explicar benefÃ­cios de cada modo
   - Mostrar comparaÃ§Ã£o de ROI

3. **AGUARDAR ESCOLHA EXPLÃCITA**
   - NÃ£o prosseguir sem decisÃ£o clara
   - Esclarecer dÃºvidas se necessÃ¡rio
   - Oferecer ver exemplos se solicitado

4. **COLETAR CONTEXTO** (se Premium)
   - InformaÃ§Ãµes sobre negÃ³cio
   - Casos de uso
   - AdaptaÃ§Ãµes desejadas

5. **CRIAR SCRIPT PYTHON** orquestrador
   - Importar todos os mÃ³dulos
   - Configurar pipeline com escolhas do usuÃ¡rio
   - Incluir logging transparente

6. **EXECUTAR CONVERSÃƒO**
   - Mostrar progresso em cada fase
   - Informar tokens usados (se Premium)
   - Validar output

7. **APRESENTAR RELATÃ“RIO**
   - EstatÃ­sticas completas
   - Custos (se Premium)
   - PrÃ³ximos passos
   - Como usar a nova skill

### EducaÃ§Ã£o ContÃ­nua

Ao apresentar opÃ§Ãµes, Claude deve:
- âœ… Explicar O QUÃŠ cada modo faz
- âœ… Explicar POR QUÃŠ as otimizaÃ§Ãµes existem
- âœ… Mostrar matemÃ¡tica transparente de custos
- âœ… Demonstrar ROI claramente
- âœ… Dar autonomia de escolha ao usuÃ¡rio

### TransparÃªncia em Custos

Sempre mostrar:
- Tokens de geraÃ§Ã£o (0 ou valor calculado)
- Custo em USD
- Tokens economizados no uso
- Break-even point
- Economia total em 100 consultas

## ğŸ¯ Casos de Uso

### Caso 1: Livro para Uso Intensivo
```
Livro: 400 pÃ¡ginas
Uso esperado: Consulta diÃ¡ria
RecomendaÃ§Ã£o: Premium
RazÃ£o: ROI positivo em 3 dias
```

### Caso 2: POC/Teste
```
Livro: 200 pÃ¡ginas
Uso esperado: ExploraÃ§Ã£o inicial
RecomendaÃ§Ã£o: BÃ¡sico
RazÃ£o: 0 custo, pode evoluir depois
```

### Caso 3: Biblioteca de Livros
```
MÃºltiplos livros: 5+ livros
Uso esperado: Variado
RecomendaÃ§Ã£o: BÃ¡sico para todos, Premium para crÃ­ticos
RazÃ£o: Otimizar investimento
```

## ğŸ“š FAQ sobre a Meta-Skill

**P: Posso converter um livro em BÃ¡sico e depois em Premium?**
R: Sim! Reconverta o mesmo PDF escolhendo Premium.

**P: A skill funciona para livros em qualquer idioma?**
R: Sim, mas o refinamento Premium funciona melhor em inglÃªs/portuguÃªs.

**P: Posso customizar as regras de conversÃ£o?**
R: Sim, edite os mÃ³dulos Python conforme necessÃ¡rio.

**P: Quanto tempo leva a conversÃ£o?**
R: BÃ¡sico: 2-5 minutos | Premium: 10-20 minutos (depende do tamanho)

**P: Posso usar em PDFs escaneados?**
R: Parcialmente. OCR pode degradar qualidade. Melhor usar PDFs com texto.

**P: A skill gerada pode ser compartilhada?**
R: Sim! Copie a pasta para /mnt/skills/user/ de outro usuÃ¡rio.

---

**ğŸ‰ Esta meta-skill transforma qualquer livro tÃ©cnico em um especialista virtual disponÃ­vel 24/7!**
